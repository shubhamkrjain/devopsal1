{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-1-88fe4326f4aa>, line 134)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-88fe4326f4aa>\"\u001b[1;36m, line \u001b[1;32m134\u001b[0m\n\u001b[1;33m    area = endX*endY\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python mask_rcnn.py --mask-rcnn mask-rcnn-coco --image images/peoplle.png\n",
    "# python mask_rcnn.py --mask-rcnn mask-rcnn-coco --image images/example_03.jpg --visualize 1\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "mind = joblib.load('distance.pk1')\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "\thelp=\"path to input image\")\n",
    "ap.add_argument(\"-m\", \"--mask-rcnn\", required=True,\n",
    "\thelp=\"base path to mask-rcnn directory\")\n",
    "ap.add_argument(\"-v\", \"--visualize\", type=int, default=0,\n",
    "\thelp=\"whether or not we are going to visualize each instance\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "\thelp=\"minimum probability to filter weak detections\")\n",
    "ap.add_argument(\"-t\", \"--threshold\", type=float, default=0.3,\n",
    "\thelp=\"minimum threshold for pixel-wise mask segmentation\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# load the COCO class labels our Mask R-CNN was trained on\n",
    "labelsPath = os.path.sep.join([args[\"mask_rcnn\"],\n",
    "\t\"object_detection_classes_coco.txt\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# load the set of colors that will be used when visualizing a given\n",
    "# instance segmentation\n",
    "colorsPath = os.path.sep.join([args[\"mask_rcnn\"], \"colors.txt\"])\n",
    "COLORS = open(colorsPath).read().strip().split(\"\\n\")\n",
    "COLORS = [np.array(c.split(\",\")).astype(\"int\") for c in COLORS]\n",
    "COLORS = np.array(COLORS, dtype=\"uint8\")\n",
    "\n",
    "# derive the paths to the Mask R-CNN weights and model configuration\n",
    "weightsPath = os.path.sep.join([args[\"mask_rcnn\"],\n",
    "\t\"frozen_inference_graph.pb\"])\n",
    "configPath = os.path.sep.join([args[\"mask_rcnn\"],\n",
    "\t\"mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\"])\n",
    "\n",
    "# load our Mask R-CNN trained on the COCO dataset (90 classes)\n",
    "# from disk\n",
    "print(\"[INFO] loading Mask R-CNN from disk...\")\n",
    "net = cv2.dnn.readNetFromTensorflow(weightsPath, configPath)\n",
    "\n",
    "# load our input image and grab its spatial dimensions\n",
    "image = cv2.imread(args[\"image\"])\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# construct a blob from the input image and then perform a forward\n",
    "# pass of the Mask R-CNN, giving us (1) the bounding box  coordinates\n",
    "# of the objects in the image along with (2) the pixel-wise segmentation\n",
    "# for each specific object\n",
    "blob = cv2.dnn.blobFromImage(image, swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "start = time.time()\n",
    "(boxes, masks) = net.forward([\"detection_out_final\", \"detection_masks\"])\n",
    "end = time.time()\n",
    "\n",
    "# show timing information and volume information on Mask R-CNN\n",
    "print(\"[INFO] Mask R-CNN took {:.6f} seconds\".format(end - start))\n",
    "print(\"[INFO] boxes shape: {}\".format(boxes.shape))\n",
    "print(\"[INFO] masks shape: {}\".format(masks.shape))\n",
    "\n",
    "# loop over the number of detected objects\n",
    "for i in range(0, boxes.shape[2]):\n",
    "\t# extract the class ID of the detection along with the confidence\n",
    "\t# (i.e., probability) associated with the prediction\n",
    "\tclassID = int(boxes[0, 0, i, 1])\n",
    "\tconfidence = boxes[0, 0, i, 2]\n",
    "\n",
    "\t# filter out weak predictions by ensuring the detected probability\n",
    "\t# is greater than the minimum probability\n",
    "\tif confidence > args[\"confidence\"]:\n",
    "\t\t# clone our original image so we can draw on it\n",
    "\t\tclone = image.copy()\n",
    "\n",
    "\t\t# scale the bounding box coordinates back relative to the\n",
    "\t\t# size of the image and then compute the width and the height\n",
    "\t\t# of the bounding box\n",
    "\t\tbox = boxes[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\tboxW = endX - startX\n",
    "\t\tboxH = endY - startY\n",
    "\n",
    "\t\t# extract the pixel-wise segmentation for the object, resize\n",
    "\t\t# the mask such that it's the same dimensions of the bounding\n",
    "\t\t# box, and then finally threshold to create a *binary* mask\n",
    "\t\tmask = masks[i, classID]\n",
    "\t\tmask = cv2.resize(mask, (boxW, boxH),\n",
    "\t\t\tinterpolation=cv2.INTER_NEAREST)\n",
    "\t\tmask = (mask > args[\"threshold\"])\n",
    "\n",
    "\t\t# extract the ROI of the image\n",
    "\t\troi = clone[startY:endY, startX:endX]\n",
    "\n",
    "\t\t# check to see if are going to visualize how to extract the\n",
    "\t\t# masked region itself\n",
    "\t\tif args[\"visualize\"] > 0:\n",
    "\t\t\t# convert the mask from a boolean to an integer mask with\n",
    "\t\t\t# to values: 0 or 255, then apply the mask\n",
    "\t\t\tvisMask = (mask * 255).astype(\"uint8\")\n",
    "\t\t\tinstance = cv2.bitwise_and(roi, roi, mask=visMask)\n",
    "\n",
    "\t\t\t# show the extracted ROI, the mask, along with the\n",
    "\t\t\t# segmented instance\n",
    "\t\t\tcv2.imshow(\"ROI\", roi)\n",
    "\t\t\tcv2.imshow(\"Mask\", visMask)\n",
    "\t\t\tcv2.imshow(\"Segmented\", instance)\n",
    "\n",
    "\t\t# now, extract *only* the masked region of the ROI by passing\n",
    "\t\t# in the boolean mask array as our slice condition\n",
    "\t\troi = roi[mask]\n",
    "\n",
    "\t\t# randomly select a color that will be used to visualize this\n",
    "\t\t# particular instance segmentation then create a transparent\n",
    "\t\t# overlay by blending the randomly selected color with the ROI\n",
    "\t\tcolor = random.choice(COLORS)\n",
    "\t\tblended = ((0.4 * color) + (0.6 * roi)).astype(\"uint8\")\n",
    "\n",
    "\t\t# store the blended ROI in the original image\n",
    "\t\tclone[startY:endY, startX:endX][mask] = blended\n",
    "\n",
    "\t\t# draw the bounding box of the instance on the image\n",
    "\t\tcolor = [int(c) for c in color]\n",
    "\t\tcv2.rectangle(clone, (startX, startY), (endX, endY), color, 2)\n",
    "        \n",
    "\t\tarea = endX*endY\n",
    "\t\tdistance = mind.predict([[area]])\n",
    "\t\tdistances = str(distance)\n",
    "\t\t# draw the predicted label and associated probability of the\n",
    "\t\t# instance segmentation on the image\n",
    "\t\ttext = \"{}: {:.4f}\".format(LABELS[classID], confidence)\n",
    "\t\tcv2.putText(clone, distances, (startX, startY - 5),\n",
    "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\t\t# show the output image\n",
    "\t\tcv2.imshow(\"Output\", clone)\n",
    "\t\tcv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
